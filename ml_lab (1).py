# -*- coding: utf-8 -*-
"""Ml lab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EB8mXW0eFO_zIuBnldi-QwgBsNiPfJKC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data_set= {
    'sqft_living':[650, 800, 1200, 1400, 1675],
    'price':[900, 120000, 150000, 19000, 345000]

}

home_data= pd.DataFrame(data_set)

x= home_data[['sqft_living']].values
y=home_data[['price']].values

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x,y,test_size=1/3, random_state=0)

from sklearn.linear_model import LinearRegression
shristi=LinearRegression()
shristi.fit(x_train, y_train)

y_test_pred=shristi.predict(x_test)
y_train_pred=shristi.predict(x_train)
print("model slope(coefficient)", shristi.coef_)
print("model intercept", shristi.intercept_)

plt.figure(figsize=(8,5))
plt.scatter(x_train, y_train,label='training data', color='green')

from sklearn.metrics import mean_squared_error, r2_score
print("\nTrain R^2:",r2_score(y_train,y_train_pred))
print("Test R^2:",r2_score(y_test,y_test_pred))
print("Test RMSE:",np.sqrt(mean_squared_error(y_test,y_test_pred)))

#Visualize the training results with a smooth regression line
plt.figure(figsize=(8,5))
plt.scatter(x_train,y_train,label='Training data',color='green')
#Create Smooth line for regression
line_x=np.linspace(x.min(),x.max(),100).reshape(-1,1)
line_y=shristi.predict(line_x)
plt.plot(line_x,line_y,color='red',linewidth=2,label='Regression line')

plt.title('House Price vs Living Area(Training set)')
plt.xlabel('Living Area(sqft)')
plt.ylabel('House Price($)')
plt.legend()
plt.grid(True)

#Visualize the test results(with same regression line)
plt.figure(figsize=(8,5))
plt.scatter(x_test,y_test,label='Test data',color='blue')
plt.plot(line_x,line_y,color='red',linewidth=2,label='Regression line')
plt.title('House Price vs Living Area(Test set)')
plt.xlabel('Living Area(sqft)')
plt.ylabel('House Price($)')
plt.legend()
plt.grid(True)
plt.show



print(df.info())

#Importing required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_curve,auc

#Load Titanic dataset
data = sns.load_dataset('titanic')
data.head()

#Display dataset info and statistics
print(data.info())
print(data.describe())

#Select Features
features=['pclass','sex','age','sibsp','parch','fare']
data=data[features + ['survived']]
data

#Handling missing values
data['age'].fillna(data['age'].median(), inplace=True)

#convert categorical column 'sex' to numerical
le= LabelEncoder()
data['sex']=le.fit_transform(data['sex'])

#Split features and target
x=data[features]
y=data['survived']

#Train-Test Split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)

#Evaluation
print("Accuracy:",accuracy_score(y_test,y_pred))
print("\nClassification Report:\n",classification_report(y_test,y_pred))
print("Confusion Matrix:\n",confusion_matrix(y_test,y_pred))

#Confusion Matrix and Graph
cm=confusion_matrix(y_test,y_pred)

plt.figure(figsize=(8,6))
sns.heatmap(cm,annot=True,fmt='d',cmap='Blues',xticklabels=['Did Not Survived','Survived'],yticklabels=['Did Not Survive','Survived'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

import pandas as pd
data={
    'study hours':[1,2,3,4,5,6,7],
    'attendance':[55,60,65,70,76,89,90],
    'internal marks':[34,67,89,9,76,45,45],
    'result':['f','p','f','p','f','f','p']
}
df=pd.DataFrame(data)
print("student performance dataset:")
print(df)
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix
x=df[['study hours','attendance','internal marks']]
y=df['result']
x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3,random_state=42)
model=DecisionTreeClassifier(criterion='entropy', random_state=0)
model.fit(x_train,y_train)
y_pred= model.predict(x_test)
print("\n Accuracy;", accuracy_score(y_test,y_pred))
print("\n Confusion Matrix:\n", confusion_matrix(y_test,y_pred))
plt.figure(figsize=(10,6))
plot_tree(model, feature_names=x.columns, class_names=['f','p'], filled=True)
plt.show()

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
iris=load_iris()
df= pd.DataFrame(iris.data, columns=iris.feature_names)
df['species']=iris.target
print("sample of dataset:")
print(df.head())
x=df.iloc[:,:-1]
y=df.iloc[:,-1]
x_train,x_test, y_train, y_test=train_test_split(x,y,test_size=0.3,random_state=42)
rf= RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(x_train,y_train)
y_pred=rf.predict(x_test)
print("\nconfusion matrix:")
print(confusion_matrix(y_test,y_pred))
print("\nclassification report:")
print(classification_report(y_test,y_pred))
print("\nAccuracy of Random Forest model:"),
print(accuracy_score(y_test,y_pred))

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
iris=load_iris()
df['species']= iris.target
print('sample of dataset:')
print(df.head())
x=df.iloc[:,:-1]
y=df.iloc[:,-1]
x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=42)
scaler=StandardScaler()
x_train_scaled=scaler.fit_transform(x_train)
x_test_scaled=scaler.transform(x_test)
knn=KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train_scaled,y_train)
y_pred=knn.predict(x_test_scaled)
print("\nconfusion matrix:")
print(confusion_matrix(y_test,y_pred))
print("\nclassification report:")
print(classification_report(y_test,y_pred))
print("\nAccuracy of KNN:", accuracy_score(y_test,y_pred))

"""linear regression: takes place when there is single dependent and independent variable
multiple regression: single independent and multiple dependent variables
polynomial regression : only one independent with degress of independent variable
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
X,y = iris.data, iris.target

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

base_model = DecisionTreeClassifier()

bag_model = BaggingClassifier(estimator=base_model,n_estimators=10,random_state=42)

base_model.fit(X_train,y_train)
bag_model.fit(X_train,y_train)

y_pred_base = base_model.predict(X_test)
y_pred_bag = bag_model.predict(X_test)

print("Accuracy of single Decision Tree:", accuracy_score(y_test, y_pred_base))
print("Accuracy after Bagging:", accuracy_score(y_test, y_pred_bag))

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
base_learners=[
    ('svm', SVC(probability=True)),
    ('knn', KNeighborsClassifier(n_neighbors=5))
]
meta_model=LogisticRegression()
stack_model=StackingClassifier(estimators=base_learners, final_estimator=meta_model)
stack_model.fit(x_train,y_train)
y_pred_stack=stack_model.predict(x_test)
print("Accuracy of Stacking:", accuracy_score(y_test, y_pred_stack))

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

iris = load_iris()
X, y = iris.data, iris.target
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


base_model=DecisionTreeClassifier(max_depth=1)
boost_model=AdaBoostClassifier(estimator=base_model,n_estimators=50, learning_rate=1.0, random_state=42)
boost_model.fit(x_train,y_train)
y_pred_boost=boost_model.predict(x_test)
print("Accuracy with adaboost", accuracy_score(y_test, y_pred_boost))

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
iris = load_iris()
X, y = iris.data, iris.target
x_train,x_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)
gb_model=GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=42)
gb_model.fit(x_train,y_train)
y_pred_gb=gb_model.predict(x_test)
print("confusion matrix:")
print(confusion_matrix(y_test,y_pred_gb))
print("\nclassification report:")
print(classification_report(y_test,y_pred_gb))
print("\naccuracy of gradient boosting:", accuracy_score(y_test,y_pred_gb))

!pip install xgboost
import xgboost as xgb
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix,classification_report
iris = load_iris()
X,y = iris.data , iris.target

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)

xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric='mlogloss'
)

"""single layer: input, output
multiple: fully connected (input, output and hidden)
relu: converts postive integer to 1 and negative to zero. [0,x]
sigmoid func: activation func
b:bias
tanh: activation func , range=[-1,1]
y=f(w1x1+w2x2+b)
bias is used to not neglect any node possible even if the smallest value.
"""

#Import Libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

#Create Simple Training Data
#Features:([Study Hours, Sleep Hours])
x=np.array([
    [2,9],
    [1,5],
    [3,6],
    [4,8],
    [6,9],
    [5,5],
    [7,3]
],)

#Labels: 1=Pass,0=Fail
y=np.array([
    [0],
    [0],
    [0],
    [1],
    [1],
    [0],
    [1]
], dtype=float)

#Normalize data
x=x/np.amax(x,axis=0)  #Scale all values between 1 and 0

#Build the Neural Network Model
model=keras.Sequential([
    layers.Dense(4,input_dim=2,activation='relu'), #Hidden layer with 4 neurons
    layers.Dense(1,activation='sigmoid'), #Output layer(binary output)

])

#Train the model
model.fit(x,y,epochs=200,verbose=0) #Train for 200 epochs quitely

#Test the model with a new input
test_data=np.array([[4,7]])  /np.amax(x,axis=0) #normalize test input
prediction=model.predict(test_data)

print("Predicted Output (1=Pass, 0=Fail):",prediction)
if prediction >=0.5:
  print("The student is likely to PASS")
else:
  print("The student is likely to FAIL")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
iris= load_iris()
x= iris.data
y=iris.target
pca= PCA(n_components=2)
x_pca= pca.fit_transform(x)
plt.figure(figsize=(7, 5))
plt.scatter(x_pca[:,0], x_pca[:,1], c=y, cmap='rainbow')
plt.title("pca:iris dataset(4 features-2 principal components)")
plt.xlabel("principal component1")
plt.ylabel("principal component")
plt.show()

#Importing Libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

#Create some sample data
X=np.array([
    [1,2],[1.5,1.8],[5,8],
    [8,8],[1,0.6],[9,11],
    [8,2],[10,2],[9,3]
])

#Build K-Means model
kmeans=KMeans(n_clusters=3,random_state=0)

#Fit the model
kmeans.fit(X)

#Get cluster centers and labels
centroids=kmeans.cluster_centers_
labels=kmeans.labels_

#Visualize the clusters
colors=["red","green","blue"]

for i in range(len(X)):
  plt.scatter(X[i][0], X[i][1],color=colors[labels[i]])

  plt.scatter(centroids[:,0],centroids[:,1],marker='x',s=200, color='black')
  plt.title("K-Means Clustering")
  plt.xlabel("X-axis")
  plt.ylabel("Y-axis")
  plt.show()

#Importing Libraries
import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram,linkage,fcluster
from sklearn.cluster import AgglomerativeClustering

#Create sample data
X=np.array([
    [1,2],[1.5,1.8],[5,8],
    [8,8],[1,0.6],[9,11],
    [8,2],[10,2],[9,3]
])

#Perform hierarchical clustering
Z=linkage(X,method='ward')  #'ward' minimizes variance

#Plot the dendogram
plt.figure(figsize=(8,5))
dendrogram(Z)
plt.title("Hierarchical Clustering Dendogram")
plt.xlabel("Data Points")
plt.ylabel("Distance")
plt.show()

#Form clusters
clusters=fcluster(Z,t=3,criterion='maxclust')

print("Cluster labels:",clusters)